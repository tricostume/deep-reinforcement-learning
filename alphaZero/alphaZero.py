from ConnectN import ConnectN

game_setting = {'size':(3,3), 'N':3}
game = ConnectN(**game_setting)

#-----------------------------------------------------------------------------------------------------------------------
game.move((0,1))
print(game.state)
print(game.player)
print(game.score)

#-----------------------------------------------------------------------------------------------------------------------
# player -1 move
game.move((0,0))
# player +1 move
game.move((1,1))
# player -1 move
game.move((1,0))
# player +1 move
game.move((2,1))

print(game.state)
print(game.player)
print(game.score)

#-----------------------------------------------------------------------------------------------------------------------

#%matplotlib notebook

from Play import Play


gameplay=Play(ConnectN(**game_setting),
              player1=None,
              player2=None)
#-----------------------------------------------------------------------------------------------------------------------
import torch
import torch.nn as nn
import torch.nn.functional as F
from math import *
import numpy as np
import random


class Policy(nn.Module):

    def __init__(self):
        super(Policy, self).__init__()

        # solution
        self.conv = nn.Conv2d(1, 16, kernel_size=2, stride=1, bias=False)

        self.size = 2 * 2 * 16
        self.fc = nn.Linear(self.size, 32)
        # layers for the policy
        self.fc_action1 = nn.Linear(32, 16)
        self.fc_action2 = nn.Linear(16, 9)

        # layers for the critic
        self.fc_value1 = nn.Linear(32, 8)
        self.fc_value2 = nn.Linear(8, 1)
        self.tanh_value = nn.Tanh()

    def forward(self, x):
        # torch.Size([1, 1, 3, 3])
        # solution
        y = F.relu(self.conv(x))
        # torch.Size([1, 16, 2, 2])

        y = y.view(-1, self.size)
        # torch.Size([1, 64])
        y = F.relu(self.fc(y))
        # torch.Size([1, 32])

        # the action head
        a = F.relu(self.fc_action1(y))
        a = self.fc_action2(a)
        # torch.Size([1, 9])

        # availability of moves
        avail = (torch.abs(x.squeeze()) != 1).type(torch.FloatTensor)
        # torch.Size([3, 3])
        avail = avail.view(-1, 9)
        # torch.Size([1, 9])
        # locations where actions are not possible, we set the prob to zero
        maxa = torch.max(a)
        # subtract off max for numerical stability (avoids blowing up at infinity)
        exp = avail * torch.exp(a - maxa)
        prob = exp / torch.sum(exp)

        # the value head
        value = F.relu(self.fc_value1(y))
        value = self.tanh_value(self.fc_value2(value))
        # return prob.view(3,3), value


policy = Policy()
#-----------------------------------------------------------------------------------------------------------------------
import MCTS

from copy import copy
import random


def Policy_Player_MCTS(game):
    mytree = MCTS.Node(copy(game))
    for _ in range(50):
        mytree.explore(policy)

    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1)

    return mytreenext.game.last_move


def Random_Player(game):
    return random.choice(game.available_moves())
#-----------------------------------------------------------------------------------------------------------------------



game = ConnectN(**game_setting)
print(game.state)
Policy_Player_MCTS(game)

#-----------------------------------------------------------------------------------------------------------------------


#% matplotlib notebook


gameplay=Play(ConnectN(**game_setting),
              player1=None,
              player2=Policy_Player_MCTS)

#-----------------------------------------------------------------------------------------------------------------------


# initialize our alphazero agent and optimizer
import torch.optim as optim

game=ConnectN(**game_setting)
policy = Policy()
optimizer = optim.Adam(policy.parameters(), lr=.01, weight_decay=1.e-4)

#-----------------------------------------------------------------------------------------------------------------------
# train our agent

from collections import deque
import MCTS

episodes = 400
outcomes = []
losses = []


import progressbar as pb

widget = ['training loop: ', pb.Percentage(), ' ',
          pb.Bar(), ' ', pb.ETA()]
timer = pb.ProgressBar(widgets=widget, maxval=episodes).start()

for e in range(episodes):

    mytree = MCTS.Node(ConnectN(**game_setting))
    vterm = []
    logterm = []

    while mytree.outcome is None:
        for _ in range(50):
            mytree.explore(policy)

        current_player = mytree.game.player
        mytree, (v, nn_v, p, nn_p) = mytree.next()
        mytree.detach_mother()


        # solution
        # compute prob* log pi 
        loglist = torch.log(nn_p)*p

        # constant term to make sure if policy result = MCTS result, loss = 0
        constant = torch.where(p>0, p*torch.log(p),torch.tensor(0.))
        logterm.append(-torch.sum(loglist-constant))

        vterm.append(nn_v*current_player)


    # we compute the "policy_loss" for computing gradient
    outcome = mytree.outcome
    outcomes.append(outcome)


    #solution
    loss = torch.sum( (torch.stack(vterm)-outcome)**2 + torch.stack(logterm) )
    optimizer.zero_grad()

    loss.backward()
    losses.append(float(loss))
    optimizer.step()

    if (e + 1) % 50 == 0:
        print("game: ", e + 1, ", mean loss: {:3.2f}".format(np.mean(losses[-20:])),
              ", recent outcomes: ", outcomes[-10:])
    del loss

    timer.update(e + 1)

timer.finish()

#-----------------------------------------------------------------------------------------------------------------------


# plot your losses

import matplotlib.pyplot as plt

#% matplotlib notebook
plt.plot(losses)
plt.show()

#-----------------------------------------------------------------------------------------------------------------------



#% matplotlib notebook

# as first player
gameplay=Play(ConnectN(**game_setting),
              player1=None,
              player2=Policy_Player_MCTS)


#-----------------------------------------------------------------------------------------------------------------------


#% matplotlib notebook

# as second player

gameplay=Play(ConnectN(**game_setting),
              player2=None,
              player1=Policy_Player_MCTS)
